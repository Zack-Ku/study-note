# 2PC
- 准备阶段，协调者发送事务请求，然后参与者直接执行，反馈结果。
- 提交阶段，统计结果、提交/中止

缺点：
1. 协调者单点故障
2. 同步事务阻塞
3. 数据不一致。第二阶段commit后，如果有执行失败，则数据不一致

# 3PC
引入超时，在第三阶段超时，引用第二阶段的提交，避免阻塞。

- CanCommit阶段，协调者发送CanCommit事务请求，参与者反馈是否能参与事务，进入预备执行状态
- PreCommit阶段，协调者发送PreCommit事务请求，参与者开始执行操作，返回执行结果。协调者决定提交/中止，返回ack
- doCommit阶段，协调者统计结果，发送doCommit，真正的事务提交，提交后ack。

注意：进入阶段三后，无论协调者出现问题，或者协调者与参与者网络出现问题，都会导致参与者无法接收到协调者发出的do Commit请求或abort请求。此时，参与者都会在等待超时之后，继续执行事务提交。

优点：降低了阻塞范围，在等待超时后协调者或参与者会中断事务。避免了协调者单点问题，阶段3中协调者出现问题时，参与者会继续提交事务。
缺陷：脑裂问题依然存在，即在参与者收到PreCommit请求后等待最终指令，如果此时协调者无法与参与者正常通信，会导致参与者继续提交事务，造成数据不一致。

# Paxos算法
https://www.cnblogs.com/esingchan/p/3917718.html
https://www.cnblogs.com/endsock/p/3480093.html
Proposer ：提议者，n个
Acceptor：决策者，奇数个
Client：产生议题者,被请求
Learner：最终决策学习者

1. 多个Proposer提出议题，最新的Proposer可以和Acceptor提议
2. Acceptor投票，少数服从多数，Proposer得到多数结果
3. 多个Proposer再次提议，向Acceptor确认投票结果，少数服从多数。
4. 最终决定Client的Proposer，然后同步信息给Learner和其他Proposer

Paxos利用的是选举，少数服从多数的思想，只要2N+1个节点中，有N个以上同意了某个决定，则认为系统达到了一致，并且按照Paxos原则，最终理论上也达到了一致，不会再改变。这样的话，客户端不必与所有服务器通信，选择与大部分通信即可；

也无需服务器都全部处于工作状态，有一些服务器挂掉，只有保证半数以上存活着，整个过程也能持续下去，容错性相当好。

因此，以前看有的博客说在部署ZooKeeper这种服务的时候，需要奇数台机器，这种说法当然有一定来源背景，比如如果是5台，那么任意客户端与任意其中3台达成一致就相当于投票结束，不过6台有何不可？只是此时需要与4台以上达成一致。 

# 拜占庭将军问题
Paxos算法如果accepor、Proposer通信不可靠，会存在拜占庭将军问题（传输有叛徒，被串改）。

# Raft算法
时间自增选主，请求提交给主，主提交复制给从，从执行成功会反馈给主，超过半数成功则认为复制成功。然后主再通知剩余的从完成同步。

# CAP 理论
Consistency，一致性。所有分布式节点的数据是否一致。
Availability，可用性。在部分节点有问题的情况（数据不一致、节点故障）下，是否能继续响应服务（可用）。
Partition tolerance，分区容错性。允许在节点（分区）数据不一致的情况。

分布式系统中，三者不能共存，P是一定存在的，就是P时，选择A或者C。

Zookeeper是CP，会卡顿，可能雪崩
Eureka是AP，数据不一致


